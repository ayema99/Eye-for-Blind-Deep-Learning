# Eye-for-Blind-Deep-Learning

To create a deep learning model that can explain the content of an image in the form of speech through caption generation with the attention mechanism on the Flickr8K data set
This type of model is a use case for blind people so that they can understand any image with the help of speech. The caption generated through a CNN-RNN model will be converted to speech using a text-to-speech library.
This problem statement is an application of both deep learning and natural language processing. The features of an image will be extracted by the CNN-based encoder, and this will be decoded by an RNN model.

According to the World Health Organization (WHO), it has been reported that there are around 285 million visually impaired people worldwide and out of these 285 million, 39 million are totally blind. It becomes really tough for these people to go through daily lifestyle work, one of which is reading. From reading a newspaper or a magazine to reading an important text message from your bank, it becomes tough for these blind people to go through the text written in it.

Imagine you went trekking recently with your family and enjoyed it a lot there. You clicked thousands of pictures there, pictures of nature, hills, forests, etc. You want to share your experience with a friend but unfortunately, he is not able to see the photos as he is blind. But you really want to make him realize what exactly the place looks like up there as it is one of his favorite trekking places. How will you be able to share your experience with him then?

Have you ever seen a blind person operate Facebook, Youtube, etc on his/her phone? Have you ever wondered how blind people can easily scroll through the news feed and understand what other people have shared across their timeline?

In 2010 Facebook launched a special feature that can help blind people operate Facebook on their respective mobile phones. The feature helped the blind person understand what he/she is typing. With the help of this feature, whenever the person tapped on a particular alphabet on the keypad, the application responded by speaking out that particular alphabet. Also, the feature could explain the blind person the contents of an image that their friends have posted on Facebook. So if someone posted a picture with their dog, the Facebook application would speak out the contents.  
